services:
  llamacpp-server:
    build:
      context: .
      dockerfile: Dockerfile
    image: cslev/llamacpp-cuda-arm64:latest
    container_name: llamacpp-server
    restart: unless-stopped
#    ports:
#      - "3000:8033"
    volumes:
      # Maps your local models folder to /models in the container
      - ./models:/models
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
    # Command optimized for ARM64 and Router Mode
    command: >
      --models-preset /models/models.ini
      --host 0.0.0.0
      --port ${LLAMACPP_PORT}
      --n-gpu-layers 99
      --ctx-size 8192
      --parallel 1
      --flash-attn on
      --context-shift

    networks:
      mydocker_network:
        ipv4_address: ${LLAMACPP_IP}

networks:
  mydocker_network:
    external: true
